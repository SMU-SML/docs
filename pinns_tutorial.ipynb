{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "255a9e4f-bfd4-421d-97a4-e9f539e405f1",
   "metadata": {},
   "source": [
    "# Tutorials of PINNs Physical Informed Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f520e96-a8b6-40fb-9181-410ff38f7425",
   "metadata": {},
   "source": [
    "This is the tutorial about the implementations of physical informed neural network for the SMU SML paper reading group.  \n",
    "In the first section, the physical informed neural network is implemented based on numpy, with details about the implementations of backpropagations and the computations of residual.  \n",
    "In the second section, the implementation is based on the pytorch package.  \n",
    "All those implementations are run on my laptop. You could move those on to ManeFrame II. The last section relates to the enviroment settings on ManeFrame II in case you want to try V100/P100 GPUs on ManeFrame II."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64340e6-28ae-4db4-a5f9-3c0565388467",
   "metadata": {},
   "source": [
    "### Section I: Implementations from scratch based on __Numpy__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f02943-9036-42e3-8f42-c51f8e800d6d",
   "metadata": {},
   "source": [
    "In this section, we build the neural networks from scratch based on the numpy package with specific physical laws (PDEs & ODEs) as regularizations.  \n",
    "\n",
    "This section is based on https://towardsdatascience.com/coding-a-neural-network-from-scratch-in-numpy-31f04e4d605, https://www.geeksforgeeks.org/implementation-of-neural-network-from-scratch-using-numpy/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010c37cd-241b-4bfa-bffd-734776ac33ad",
   "metadata": {},
   "source": [
    "Let's give a short review first.  \n",
    "\n",
    "The feed forward neural network, or deep neural network is the mathematical model of neural connections in brain, with trainable weights and activation functions with bias that simulate the intensities of the connections between each neuron, or the concentrations of the neurotransmitters at the synapse between each neuron, and the activation with specific threshold of every neuron, respectively. (Biological terminologies are from https://www.inf.ed.ac.uk/teaching/courses/nlu/assets/reading/Gurney_et_al.pdf, this is a really nice book). Mathematically, The architecture of neural network with one neuron has form\n",
    "$$\n",
    "A = \\sigma(\\sum_i^n\\omega_i*x_i + b),\n",
    "$$\n",
    "where $\\omega_i, i=1\\ldots n$ are the weights (the intensities of the connections between each neuron), and $\\sigma$ is the activation function with threshold $b$. \n",
    "\n",
    "And the neurons could also be sumed with several trainable weights\n",
    "$$\n",
    "A = \\sum_j^m W_j\\sigma(\\sum_i^n\\omega_{ij}x_i + b_j)\n",
    "$$\n",
    "which is a layer of neurons. The neurons are called the hidden neurons, thus the layer may also be refered as hidden layers.\n",
    "\n",
    "Thus, the deep neural network are the stack of several hidden layers,\n",
    "$$\n",
    "\\tag{1}\n",
    "A = \\sum_j^m W_j\\sigma(\\ldots \\sigma(\\sum_i^n\\omega_{i\\cdot}x + b_\\cdot)+ \\ldots + b_j)\n",
    "$$\n",
    "where the $\\cdot$ in the inner most layer means the index of the neurons in the next layer.\n",
    "For the last layer, or the layer intermediate to the output, sometimes we get rid of the bias for that layer, as shown in Eq. (1). \n",
    "\n",
    "We could also write these architecture by matrix form\n",
    "$$\\tag{2}\n",
    "A = \\boldsymbol{W_n}\\sigma(\\ldots \\sigma(\\boldsymbol{W}_1 x + \\boldsymbol{b}_1) +\\ldots + \\boldsymbol{b}_n)\n",
    "$$\n",
    "with $\\boldsymbol{W}_n$ as the weight matricies and $\\boldsymbol{b}_n$ as the bias vectors.\n",
    "\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20200522110034/NURELNETWORK.jpg\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "*The schematics of a 1 hidden layer neral network with 5 hidden neurons, 30 input dimentions, and 3 output dimensions. The weights $\\boldsymbol{W}_1$ for the input $\\boldsymbol{x}$ is a $30\\times 5$ matrix. The bias vector $\\boldsymbol{b}_1$ has shape $5\\times 1$. The weights $\\boldsymbol{W}_2$ for the output has shape $5\\times 3$.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed615ab-66f4-452a-9246-073bf691ff30",
   "metadata": {},
   "source": [
    "Now, We build the neural networks by numpy from scratch to give an intuitive understanding about the backpropagations. To simplify the coding, we only implement the neural network with 1 hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9c85a6-861e-4cd7-b915-a6c0acce2aa9",
   "metadata": {},
   "source": [
    "#### Activation Functions\n",
    "First, we implement the activation function, the considered activation function is sigmoid(x)\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\dfrac{1}{1+e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746c1041-a3ab-4a68-a81b-152740f57758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf6bd44-5f50-4832-94cb-71eb99efc966",
   "metadata": {},
   "source": [
    "#### Creating the Feed forward neural network\n",
    "\n",
    "Now we consider a 1 hidden layer neural network with 50 hidden neurons in the hidden layer.\n",
    "- 1 Input layer (1)\n",
    "- 1 hidden layer (50)\n",
    "- 1 output layer (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de7e96d-bffb-4f8f-b654-a5227da6a006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_forward(t, w1, b1, w2):\n",
    "    # hidden\n",
    "    z1 = t.dot(w1) + b1# input from layer 1\n",
    "    a1 = sigmoid(z1)# out put of layer 2\n",
    "\n",
    "    # Output layer\n",
    "    a2 = a1.dot(w2)# input of out layer\n",
    "    return a2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b3944-8d83-4047-98a1-c91314fa7b51",
   "metadata": {},
   "source": [
    "The problem we consider to solve is a first-order ode\n",
    "$$ \n",
    "u^{\\prime} = u, \\;\\;\\; t \\in [0,1],\n",
    "$$\n",
    "with initial condition $u(0) = 1$\n",
    "\n",
    "Let $u_{\\theta}$ be the neural network with parameters $\\theta$ sovling the ode. Thus, the corresponding loss we considered has 2 part, the residual $\\int_0^1 (u_{\\theta}^{\\prime} - u_{\\theta})^2dt$ and the boundary loss $(u_{\\theta}(0) - 1)^2$. The residual is approximated by \n",
    "$$\n",
    "\\int_0^1 (u_{\\theta}^{\\prime} - u_{\\theta})^2 dt \\approx \\dfrac{1}{n}\\sum_i (u_\\theta^{\\prime}(t_i) - u_\\theta(t_i))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8ddc7f-b2ae-4937-b8ef-5d46cae9a25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(u, u_prime, u0):\n",
    "    # Residual\n",
    "    s =(np.square(u - u_prime))\n",
    "    s = np.mean(s)\n",
    "    # boundary loss\n",
    "    s = s + np.square(u0-1.)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90b6f95-23a3-475b-98cf-93eb247fb7e9",
   "metadata": {},
   "source": [
    "To compute the derivative $\\dfrac{\\partial }{\\partial t} u_\\theta$, we could first derive it by hand\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\dfrac{\\partial }{\\partial t} u_\\theta & = \\dfrac{\\partial }{\\partial t} \\boldsymbol{W_2}\\sigma(\\boldsymbol{W}_1 t + \\boldsymbol{b}_1) \\\\\n",
    "                                       & = \\boldsymbol{W_2}[\\sigma^{\\prime}(\\boldsymbol{W}_1 t + \\boldsymbol{b}_1)\\odot \\boldsymbol{W}_{1}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "with $\\odot$ as the elementwise multiplications.\n",
    "\n",
    "Note $\\sigma^{\\prime}(x) = \\sigma(x)\\cdot(1-\\sigma(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52930eef-51ea-4457-967d-f12cac47a83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dudt(t, W1,b1,W2):\n",
    "    # hidden layer\n",
    "    z1 = t.dot(w1)+b1# input from layer 1\n",
    "    a1 = sigmoid(z1)# output of hidden Layer\n",
    "    InSquareBracket = np.multiply(np.multiply(a1, 1-a1), W1) # \\sigma^{\\prime}(W_1 x + b_1)\\odot W_1\n",
    "    u_t = InSquareBracket.dot(w2)\n",
    "    return u_t   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bef2c18-4667-4e12-af9a-8b7c00831789",
   "metadata": {},
   "source": [
    "Now we implement the backpropagation with respect to the parameters $\\boldsymbol{W}_1, \\boldsymbol{b}_1, \\boldsymbol{W}_2$.\n",
    "\n",
    "According to the loss, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}(\\theta) &= (u_{\\theta}(0) - 1)^2 + \\dfrac{1}{n}\\sum_i (  u_\\theta^{\\prime}(t_i) - u_\\theta(t_i))^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "To find the best opproximation, we consider to minimize the loss by gradient descent method. \n",
    "\n",
    "The derivative of loss $\\mathcal{L}$ with respect to $\\boldsymbol{W}_1, \\boldsymbol{b}_1, \\boldsymbol{W}_2$ should have form\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\dfrac{\\partial}{\\partial \\boldsymbol{W}_1} \\mathcal{L}(\\theta) &= \\dfrac{\\partial}{\\partial \\boldsymbol{W}_1}\\left[ (\\boldsymbol{W_2}\\sigma(\\boldsymbol{W}_1 0 + \\boldsymbol{b}_1) - 1)^2 + \\dfrac{1}{n}\\sum_i (  \\boldsymbol{W_2}[\\sigma^{\\prime}(\\boldsymbol{W}_1 t_i + \\boldsymbol{b}_1)\\odot \\boldsymbol{W}_{1}] - \\boldsymbol{W_2}\\sigma(\\boldsymbol{W}_1 t_i + \\boldsymbol{b}_1))^2\\right]\\\\\n",
    "& = \\dfrac{1}{n}\\sum_i 2 \\left\\{ \\boldsymbol{W_2}[\\sigma^{\\prime}(\\boldsymbol{W}_1 t_i + \\boldsymbol{b}_1)\\odot \\boldsymbol{W}_{1}] - \\boldsymbol{W_2}\\sigma(\\boldsymbol{W}_1 t_i + \\boldsymbol{b}_1)\\right\\} \\left\\{ \\boldsymbol{W_2}\\odot[t_i \\sigma^{\\prime\\prime}(\\boldsymbol{W}_1 t_i + \\boldsymbol{b}_1)\\odot \\boldsymbol{W}_{1} + \\sigma^{\\prime}(\\boldsymbol{W}_1 t_i + \\boldsymbol{b}_1)] - \\boldsymbol{W_2}\\odot\\sigma^{\\prime}(\\boldsymbol{W}_1 t_i + \\boldsymbol{b}_1)t_i\\right\\}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\dfrac{\\partial}{\\partial \\boldsymbol{b}_1} \\mathcal{L}(\\theta) &= \\dfrac{\\partial}{\\partial \\boldsymbol{b}_1}\\left[ (\\boldsymbol{W_2}\\sigma(\\boldsymbol{W}_1 0 + \\boldsymbol{b}_1) - 1)^2 + \\dfrac{1}{n}\\sum_i (  \\boldsymbol{W_2}[\\sigma^{\\prime}(\\boldsymbol{W}_1 t_i + \\boldsymbol{b}_1)\\odot \\boldsymbol{W}_{1}] - \\boldsymbol{W_2}\\sigma(\\boldsymbol{W}_1 t_i + \\boldsymbol{b}_1))^2\\right]\\\\\n",
    "& = 2 (\\boldsymbol{W_2}\\sigma(\\boldsymbol{b}_1) - 1)(\\boldsymbol{W_2}\\odot\\sigma^{\\prime}(\\boldsymbol{b}_1)) + \\dfrac{1}{n}\\sum_i 2 \\left\\{  \\boldsymbol{W_2}[\\sigma^{\\prime}(\\boldsymbol{W}_1 t_i + \\boldsymbol{b}_1)\\odot \\boldsymbol{W}_{1}] - \\boldsymbol{W_2}\\sigma(\\boldsymbol{W}_1 t_i + \\boldsymbol{b}_1)\\right\\} \\left\\{   \\boldsymbol{W_2}\\odot[ \\sigma^{\\prime\\prime}(\\boldsymbol{W}_1 t_i + \\boldsymbol{b}_1)\\odot \\boldsymbol{W}_{1}]  - \\boldsymbol{W_2}\\odot\\sigma^{\\prime}(\\boldsymbol{W}_1 t_i + \\boldsymbol{b}_1)\\right\\}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\dfrac{\\partial}{\\partial \\boldsymbol{W}_2} \\mathcal{L}(\\theta) &= \\dfrac{\\partial}{\\partial \\boldsymbol{W}_2}\\left[ (\\boldsymbol{W_2}\\sigma(\\boldsymbol{W}_1 0 + \\boldsymbol{b}_1) - 1)^2 + \\dfrac{1}{n}\\sum_i (  \\boldsymbol{W_2}[\\sigma^{\\prime}(\\boldsymbol{W}_1 t_i + \\boldsymbol{b}_1)\\odot \\boldsymbol{W}_{1}] - \\boldsymbol{W_2}\\sigma(\\boldsymbol{W}_1 t_i + \\boldsymbol{b}_1))^2\\right]\\\\\n",
    "& = 2 (\\boldsymbol{W_2}\\sigma(\\boldsymbol{b}_1) - 1)(\\sigma^{}(\\boldsymbol{b}_1)) + \\dfrac{1}{n}\\sum_i 2 \\left\\{  \\boldsymbol{W_2}[\\sigma^{\\prime}(\\boldsymbol{W}_1 t_i + \\boldsymbol{b}_1)\\odot \\boldsymbol{W}_{1}] - \\boldsymbol{W_2}\\sigma(\\boldsymbol{W}_1 t_i + \\boldsymbol{b}_1)\\right\\} \\left\\{   [ \\sigma^{\\prime}(\\boldsymbol{W}_1 t_i + \\boldsymbol{b}_1)\\odot \\boldsymbol{W}_{1}]  - \\sigma^{}(\\boldsymbol{W}_1 t_i + \\boldsymbol{b}_1)\\right\\}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73839d19-4132-4abd-ae13-99bd9f9dfe85",
   "metadata": {},
   "source": [
    "**I strongly recommend you to derive those derivatives!** \n",
    "\n",
    "Now by change of variables, \n",
    "   $$\n",
    "    \\begin{aligned}\n",
    "        a_i &= \\sigma \\left( \\boldsymbol{W}_{1} t_{i} + \\boldsymbol{b}_{1} \\right)\\\\\n",
    "        \\sigma^{\\prime}\\left( \\boldsymbol{W}_{1}t_{i} + \\boldsymbol{b}_{1} \\right) &= a_{i}\\odot\\left( 1-a_{i} \\right)\\\\\n",
    "        \\sigma^{\\prime\\prime}\\left( \\boldsymbol{W}_{1}t_{i} + \\boldsymbol{b}_{1} \\right) &= \\left( 1-2a_{i} \\right)\\odot a_{i}\\odot\\left( 1-a_{i} \\right)\\\\\n",
    "     \\end{aligned}\n",
    "    $$\n",
    "we could see the derivative more clearly.\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial}{\\partial \\boldsymbol{W}_1} \\mathcal{L}(\\theta) = \\dfrac{1}{n} \\sum_i 2 \\left\\{ \\boldsymbol{W_2}\\cdot[a_i\\odot(1-a_i) \\odot \\boldsymbol{W}_{1}] - \\boldsymbol{W_2}\\cdot a_i\\right\\} \\left\\{ \\boldsymbol{W_2}\\odot[t_i\\left( 1-2a_{i} \\right)\\odot a_{i}\\odot\\left( 1-a_{i} \\right)\\odot \\boldsymbol{W}_{1} + a_{i}\\odot\\left( 1-a_{i} \\right)] - \\boldsymbol{W_2}\\odot a_{i}\\odot\\left( 1-a_{i} \\right)t_i\\right\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial}{\\partial \\boldsymbol{b}_1} \\mathcal{L}(\\theta) =  2 (\\boldsymbol{W_2}\\cdot\\sigma(\\boldsymbol{b}_1) - 1)\\boldsymbol{W_2}\\odot\\sigma^{\\prime}(\\boldsymbol{b}_1) + \\dfrac{1}{n}\\sum_i 2 \\left\\{  \\boldsymbol{W_2}\\cdot[a_{i}\\odot\\left( 1-a_{i} \\right)\\odot \\boldsymbol{W}_{1}] - \\boldsymbol{W_2}\\cdot a_i\\right\\} \\left\\{   \\boldsymbol{W_2}\\odot \\left( 1-2a_{i} \\right)\\odot a_{i}\\odot\\left( 1-a_{i} \\right)\\odot \\boldsymbol{W}_{1}  - \\boldsymbol{W_2}\\odot a_{i}\\odot\\left( 1-a_{i} \\right)\\right\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial}{\\partial \\boldsymbol{W}_2} \\mathcal{L}(\\theta) = 2 (\\boldsymbol{W_2}\\cdot\\sigma(\\boldsymbol{b}_1) - 1)\\sigma^{}(\\boldsymbol{b}_1) + \\dfrac{1}{n}\\sum_i 2 \\left\\{  \\boldsymbol{W_2}\\cdot[a_{i}\\odot\\left( 1-a_{i} \\right)\\odot \\boldsymbol{W}_{1}] - \\boldsymbol{W_2}\\cdot a_i\\right\\} \\left\\{   a_{i}\\odot\\left( 1-a_{i} \\right)\\odot \\boldsymbol{W}_{1}  - a_i\\right\\}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6270cd2-d5ec-4a40-91a1-1bd3eac0a6c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "Furthermore, Let \n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        \\boldsymbol{a} &= \\sigma \\left( \\boldsymbol{W}_{1} \\boldsymbol{t}+ \\boldsymbol{b}_{1} \\right)\\\\\n",
    "        \\boldsymbol{o} &= \\boldsymbol{W}_{2}\\cdot \\boldsymbol{a}\\\\\n",
    "        d_r&=  2 \\boldsymbol{a}\\odot \\left( 1-\\boldsymbol{a} \\right) \\odot \\boldsymbol{W}_{1}\\cdot \\boldsymbol{W}_{2} - \\boldsymbol{o}\\\\\n",
    "        dd_r&= \\boldsymbol{W}_{2}^{T}\\odot W_1 \\odot \\left( 1- 2 \\boldsymbol{a} \\right) \\odot \\boldsymbol{a}\\odot \\left( 1-\\boldsymbol{a} \\right)\\\\\n",
    "        w2_{dt} &= \\boldsymbol{W}_{2}^{T}\\odot \\boldsymbol{a} \\odot \\left( 1-\\boldsymbol{a} \\right)\n",
    "     \\end{aligned}\n",
    "$$\n",
    "\n",
    "Note here $\\boldsymbol{a}$ is a matrix with shape (batch_size, 50), so the elementwise multiplication of $\\boldsymbol{a}$ with $\\boldsymbol{W}$ should be understood as that $\\boldsymbol{W}$ is broadcastting along the batch axis first ( from shape (1,50) -> (batch_size, 50)), then do the elementwise multiplication with $\\boldsymbol{a}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237cd700-4f73-41fb-92e1-b12835ed0bb7",
   "metadata": {},
   "source": [
    "__When implementing those structures, keep eyes on the shape of the inputs and outputs!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c63a6c-a324-4f21-8943-6d25b97cab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   \n",
    "# Back propagation of error\n",
    "def back_prop(t, t0, w1, b1, w2, alpha):\n",
    "    # t  shape: batch_size x 1\n",
    "    # W1 shape: 1 x 50\n",
    "    # b1 shape: 1 x 50\n",
    "    # W2 shape: 50 x 1\n",
    "     \n",
    "    # hidden layer\n",
    "    z1 = t.dot(w1) + b1# input from layer 1: batch_size x 50\n",
    "    a = sigmoid(z1)# output of layer 2: batch_size x 50\n",
    "     \n",
    "    # Output layer\n",
    "    o = a.dot(w2)# input of out layer: batch_size x 1\n",
    "\n",
    "    \n",
    "    # Initial Condition\n",
    "    z10 = t0.dot(w1) + b1\n",
    "    a0 = sigmoid(z10)\n",
    "    o0 = a0.dot(w2)\n",
    "    \n",
    "    \n",
    "    # difference in the output layer from initial conditions\n",
    "    d_i = 2*(o0 - 1) # shape 1 x 1\n",
    "    \n",
    "    # difference in the output layer from the residual\n",
    "    d_r = 2*(np.multiply(np.multiply(a, 1-a), w1).dot(w2) - o) # shape: batch_size x 1\n",
    "    dd_r = np.multiply(w2.transpose(), np.multiply(np.multiply(np.multiply(1- 2*a, a), 1 - a), w1)) # shape: batch_size x 50\n",
    "    w2_dt = np.multiply(w2.transpose(), np.multiply(a, 1-a))\n",
    "    dsigmab = np.multiply(sigmoid(b1), 1- sigmoid(b1))\n",
    "    \n",
    "    \n",
    "    # Gradient for w1 and w2\n",
    "    dL_dw1= np.mean( np.multiply(d_r, np.multiply(dd_r, t) + np.multiply(w2_dt, 1-t)) ,axis=0, keepdims=True)\n",
    "    dL_db1= np.mean( np.multiply(d_r, dd_r - w2_dt), axis=0, keepdims=True) + d_i*np.multiply(w2.transpose(), dsigmab)\n",
    "    dL_dw2= (np.mean(np.multiply(d_r, np.multiply(np.multiply(a, 1-a), w1) - a), axis=0, keepdims=True) + d_i*sigmoid(b1)).reshape(-1,1)\n",
    "     \n",
    "    # Updating parameters\n",
    "    w1 = w1-(alpha*(dL_dw1))\n",
    "    w2 = w2-(alpha*(dL_dw2))\n",
    "    b1 = b1-(alpha*(dL_db1))\n",
    "\n",
    "\n",
    "    return w1,b1,w2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03daadc3-63ec-4d36-aee7-f1bb9c506a2f",
   "metadata": {},
   "source": [
    "Now we could implement the train function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef28353a-1c2a-4f3b-a8e1-ede90c296841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(w1, b1, w2, alpha=0.01, epoch=100):\n",
    "    losss = []\n",
    "    t0 = np.zeros((1,1)) # Initial conditions\n",
    "    # Let we assume the batch_size is 100, for every epoch, the neural network train 20 steps\n",
    "    batch_size = 100\n",
    "    steps = 40\n",
    "    for j in range(epoch):\n",
    "        l = []\n",
    "        for i in range(steps):\n",
    "            t = np.random.uniform(size=(batch_size,1)) # generate random t\n",
    "            u = f_forward(t, w1, b1, w2)\n",
    "            uprime = dudt(t, w1, b1, w2)\n",
    "            u0 = f_forward(t0, w1, b1, w2) # initial conditions\n",
    "            l.append(loss(u, uprime, u0))\n",
    "            w1, b1, w2 = back_prop(t, t0, w1, b1, w2, alpha)\n",
    "        print(\"epochs:\", j + 1, \"======== Loss:\", ((sum(l)/len(l))))  \n",
    "        losss.append(sum(l)/len(l))\n",
    "    return losss, w1, b1, w2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89452061-5212-4ca8-8971-952c57587e9b",
   "metadata": {},
   "source": [
    "Now we initialize the weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f065bc9-77fb-410a-b831-3017a64e5fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the weights randomly\n",
    "def generate_wt(x, y):\n",
    "    l =[]\n",
    "    for i in range(x * y):\n",
    "        l.append(np.random.randn())\n",
    "    return(np.array(l).reshape(x, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294625b4-05a3-4f6a-aa18-0e42a65de01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = generate_wt(1, 50)\n",
    "b1 = generate_wt(1, 50)\n",
    "w2 = generate_wt(50, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff35a37-67d9-4cb7-a0f6-f4bf5f4115b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w1.shape)\n",
    "print(w2.shape)\n",
    "print(b1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef8b48e-601d-42ee-a625-15c29f1485d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(w1,b1, w2):\n",
    "    t = np.linspace(0,1,100).reshape(100,1)\n",
    "    out = f_forward(t, w1, b1, w2)\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure()\n",
    "    plt.plot(t, np.exp(t), label='Exact')\n",
    "    plt.plot(t, out,'--', label='pred')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8eefdc-fa29-4ae9-9526-9dbf9a708996",
   "metadata": {},
   "source": [
    "train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59655de5-cf90-4614-a800-b239bc57f110",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "losss ,w1, b1, w2 = train(w1, b1, w2,alpha=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e158e512-ecc2-4522-902b-3b03055c3465",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred(w1,b1,w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f05925-8de4-4bce-b0d7-15de8c9fb620",
   "metadata": {},
   "source": [
    "### Section II: Implementaions based on __Pytorch__\n",
    "For this section, I assume you know how to install pyptorch and has some basic knowledge about python class & method & module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08e06a31-3ca0-43f1-8324-c5a198982ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc5012c-4e21-4dc8-8559-1f14fef19b97",
   "metadata": {},
   "source": [
    "Since the pytorch package has already packed the hidden layers we metioned above by `nn.linear`, we could build the neural network by `nn.Sequential`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a464465-ad63-4eab-9f6b-1640195eb5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(1,50),\n",
    "        torch.nn.Sigmoid(),\n",
    "        torch.nn.Linear(50,1, bias=False)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0b6f26-98e3-4d46-9e90-71369d082e5f",
   "metadata": {},
   "source": [
    "Now we define the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e82e87e-0016-4617-8fa5-ba40429a363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss(t, u0,t0):\n",
    "    upred = model(t)\n",
    "    # the u' is computed by torch.autograd.grad\n",
    "    u_prime = torch.autograd.grad(upred, t, grad_outputs = torch.ones_like(t), retain_graph=True, create_graph=True)\n",
    "    # Note the create_graph and retain_graph attributes are really important\n",
    "    residual = torch.mean(torch.square(u_prime[0] - upred))\n",
    "    u0pred = model(t0)\n",
    "    init_loss = torch.mean(torch.square(u0pred - u0))\n",
    "    return residual + 10*init_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db666b9c-d159-4f59-9bbe-a87ab2d43266",
   "metadata": {},
   "source": [
    "Note for the pytorch package, we could compute the gradient by the `torch.autograd.grad` method. In this case, the `grad_outputs=` works as the vector in the jacobian vector product. \n",
    "\n",
    "Mathematically speaking, without setting `grad_outputs=`, the method `torch.autograd.grad` compute the jacobian of vector function $u_\\theta(t0), u_\\theta(t1), \\ldots, u_\\theta(t_n) $ with respect to each $t_i$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    \\dfrac{\\partial u_{\\theta}\\left( t_0 \\right)}{\\partial t_0} & \\dfrac{\\partial u_{\\theta}\\left( t_0 \\right)}{\\partial t_1} & \\ldots & \\dfrac{\\partial u_{\\theta}\\left( t_{0} \\right)}{\\partial t_n}\\\\\n",
    "    \\dfrac{\\partial u_{\\theta}\\left( t_1 \\right)}{\\partial t_0} & \\dfrac{\\partial u_{\\theta}\\left( t_1 \\right)}{\\partial t_1} & \\ldots & \\dfrac{\\partial u_{\\theta}\\left( t_{1} \\right)}{\\partial t_n}\\\\\n",
    "    \\vdots & \\vdots &\\ddots & \\vdots\\\\\n",
    "    \\dfrac{\\partial u_{\\theta}\\left( t_n \\right)}{\\partial t_0} & \\dfrac{\\partial u_{\\theta}\\left( t_n \\right)}{\\partial t_1} & \\ldots & \\dfrac{\\partial u_{\\theta}\\left( t_{n} \\right)}{\\partial t_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here, I did some simplifications of the notiation, to be more precisely, every partial derivative is the value \n",
    "$$\n",
    "\\dfrac{\\partial u_{\\theta}\\left( t_i \\right)}{\\partial t_j} = \\left.\\dfrac{\\partial u_{\\theta}\\left( t_i \\right)}{\\partial t_j}\\right|_{t_i = t_i}\n",
    "$$\n",
    "i.e., the value of $\\dfrac{\\partial u_{\\theta}\\left( t_i \\right)}{\\partial t_j}$ at point $t_i$. \n",
    "\n",
    "So we may observe most of the components in the jacobian is $0$. By multiplying the `grad_outputs=torch.ones_like(t)` which is the gradient from the previous layer with respect to the input, shown as the vector on the right hand side, \n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    \\dfrac{\\partial u_{\\theta}\\left( t_0 \\right)}{\\partial t_0} & \\dfrac{\\partial u_{\\theta}\\left( t_0 \\right)}{\\partial t_1} & \\ldots & \\dfrac{\\partial u_{\\theta}\\left( t_{0} \\right)}{\\partial t_n}\\\\\n",
    "    \\dfrac{\\partial u_{\\theta}\\left( t_1 \\right)}{\\partial t_0} & \\dfrac{\\partial u_{\\theta}\\left( t_1 \\right)}{\\partial t_1} & \\ldots & \\dfrac{\\partial u_{\\theta}\\left( t_{1} \\right)}{\\partial t_n}\\\\\n",
    "    \\vdots & \\vdots &\\ddots & \\vdots\\\\\n",
    "    \\dfrac{\\partial u_{\\theta}\\left( t_n \\right)}{\\partial t_0} & \\dfrac{\\partial u_{\\theta}\\left( t_n \\right)}{\\partial t_1} & \\ldots & \\dfrac{\\partial u_{\\theta}\\left( t_{n} \\right)}{\\partial t_n}\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "\\dfrac{\\partial t_0}{\\partial t_0} \\\\\n",
    "\\dfrac{\\partial t_1}{\\partial t_1} \\\\\n",
    "\\vdots\\\\\n",
    "\\dfrac{\\partial t_{n}}{\\partial t_{n}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "we could obtain the gradient from the outputs of the model to the inputs of the model. (Recall the **chain rule** here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e7a758-3a2b-4e98-a56c-3d7e4de6d0dc",
   "metadata": {},
   "source": [
    "#### torch.autograd.grad\n",
    "\n",
    "Note the `create_graph` & `retain_graph` attributes in the `torch.autograd.grad(upred, t, grad_outputs = torch.ones_like(t), retain_graph=True, create_graph=True)`, both of them need to be true. \n",
    "Plainly speaking, pytorch creates a dynamic graph when computing the gradients, which means it will be destroyed once the computation is finished. For the problems with PDE as loss, we need to compute the gradient of the outputs with respect to the input, the dynamic graph will be created during computation, once we obtain the gradients, the graph is destroyed. After this, every time you call `torch.autograd.grad(upred, t, grad_outputs = torch.ones_like(t), retain_graph=True, create_graph=True)`, pytorch will not create a graph. \n",
    "\n",
    "So, we need to retain the created graph for further computation like backpropagation with respect to the parameters, thus set `retain_graph=True`.\n",
    "\n",
    "On the other hand, if the `create_graph` is false, the created graph will not be put into the dynamic computational graph of the neural network, which means the residual will not be back propagated to every parameters even though the residual is computed.\n",
    "\n",
    "In the code block below, it is a very fast verification for the case with `create_graph=False`, `l.backward()` is the step that computing the gradients for each parameters. The computed gradients are stored in the `param.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "232082f7-5bab-43a0-8a63-fc9a2a91aecb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [46], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m l \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(u_prime[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 8\u001b[0m \u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m([param\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters()])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.02)\n",
    "t = torch.rand((batch_size,1))\n",
    "t.requires_grad=True\n",
    "upred = model(t)\n",
    "u_prime = torch.autograd.grad(upred, t, grad_outputs = torch.ones_like(t), retain_graph=True)\n",
    "l = torch.mean(u_prime[0])\n",
    "optimizer.zero_grad()\n",
    "l.backward()\n",
    "print([param.grad for param in model.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4343a905-06f4-432a-855d-af88a32c8cd0",
   "metadata": {},
   "source": [
    "Note the runtimeError, since the graph of residual with respect to the parameters are not created, it reports the error that the parameter tensors does not require grad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746a91d3-d69f-4720-9ddf-69b186146a1a",
   "metadata": {},
   "source": [
    "Now we could train our model, below gives the implementation of gradient descent we mentioned above, but we will use the `nn.optim` package to train the `model`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bde978-4638-4c9f-9b86-e9e6f8beade0",
   "metadata": {},
   "source": [
    "```python\n",
    "def train(model,epochs):\n",
    "    steps = 30\n",
    "    batch_size = 100\n",
    "    t0 = torch.zeros((1,1))\n",
    "    u0 = torch.zeros((1,1))\n",
    "    for j in range(epochs):\n",
    "        for i in range(steps):\n",
    "            t = torch.rand((batch_size,1))\n",
    "            loss = Loss(model, t, u0, t0)\n",
    "            \n",
    "            # Zero the gradients before running the backward pass.\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "            # parameters of the model. Internally, the parameters of each Module are stored\n",
    "            # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "            # all learnable parameters in the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "            # we can access its gradients.\n",
    "            with torch.no_grad():            \n",
    "                for param in model.parameters():\n",
    "                    param -= learning_rate * param.grad   \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5974b487-e541-40a9-93d5-d3723dfb36a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    epochs=50\n",
    "    steps = 100\n",
    "    batch_size = 100\n",
    "    t0 = torch.zeros((1,1))\n",
    "    u0 = torch.ones((1,1))\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = 0.02)\n",
    "    for j in range(epochs):\n",
    "        L = []\n",
    "        for i in range(steps):\n",
    "            # since we need to compute the gradient with respec to t\n",
    "            # we need to set the attribute requires_grad=True\n",
    "            t = torch.rand((batch_size,1))\n",
    "            t.requires_grad=True\n",
    "            l = Loss(t, u0, t0)\n",
    "\n",
    "            # Zero the gradients before running the backward pass.\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            L.append(l.item())\n",
    "        print('Epoch:{}=====Loss:{}'.format(j + 1, sum(L)/len(L)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "95ecd7a9-c0ae-4ec4-8fb9-0df76bcd3c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model):\n",
    "    t = torch.linspace(0, 1, 100).view((-1,1))\n",
    "    pred = model(t)\n",
    "    exact = torch.exp(t)\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure()\n",
    "    plt.plot(t, exact, label='Exact')\n",
    "    plt.plot(t, pred.detach().numpy(),'--', label='pred')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b7c35e4-6b3f-4103-95d5-976e6b9af437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1=====Loss:0.004074094956740737\n",
      "Epoch:2=====Loss:0.0038685844046995044\n",
      "Epoch:3=====Loss:0.0039007688965648412\n",
      "Epoch:4=====Loss:0.003718803892843425\n",
      "Epoch:5=====Loss:0.003698199603240937\n",
      "Epoch:6=====Loss:0.0035428230254910885\n",
      "Epoch:7=====Loss:0.0034429417038336395\n",
      "Epoch:8=====Loss:0.0034027533046901226\n",
      "Epoch:9=====Loss:0.0033110639383085072\n",
      "Epoch:10=====Loss:0.0032857614755630493\n",
      "Epoch:11=====Loss:0.0032457706658169626\n",
      "Epoch:12=====Loss:0.003125250298762694\n",
      "Epoch:13=====Loss:0.0031054871040396394\n",
      "Epoch:14=====Loss:0.0029810819355770945\n",
      "Epoch:15=====Loss:0.0029422681347932664\n",
      "Epoch:16=====Loss:0.002984375755768269\n",
      "Epoch:17=====Loss:0.0029689159709960222\n",
      "Epoch:18=====Loss:0.0028361188957933335\n",
      "Epoch:19=====Loss:0.002828492691041902\n",
      "Epoch:20=====Loss:0.0027768662804737686\n",
      "Epoch:21=====Loss:0.002697467702673748\n",
      "Epoch:22=====Loss:0.0026630342181306332\n",
      "Epoch:23=====Loss:0.0025966416264418513\n",
      "Epoch:24=====Loss:0.002580357356928289\n",
      "Epoch:25=====Loss:0.0025786387792322784\n",
      "Epoch:26=====Loss:0.0024735858535859735\n",
      "Epoch:27=====Loss:0.0024951885733753443\n",
      "Epoch:28=====Loss:0.002448404476745054\n",
      "Epoch:29=====Loss:0.0023550649092067034\n",
      "Epoch:30=====Loss:0.002326355853583664\n",
      "Epoch:31=====Loss:0.0022744213277474047\n",
      "Epoch:32=====Loss:0.002280865985667333\n",
      "Epoch:33=====Loss:0.0022489153605420144\n",
      "Epoch:34=====Loss:0.002241399242775515\n",
      "Epoch:35=====Loss:0.002160061973845586\n",
      "Epoch:36=====Loss:0.0020992124860640614\n",
      "Epoch:37=====Loss:0.0020657071575988085\n",
      "Epoch:38=====Loss:0.0020502256450708957\n",
      "Epoch:39=====Loss:0.0020422755123581738\n",
      "Epoch:40=====Loss:0.0020380339096300304\n",
      "Epoch:41=====Loss:0.0020263822237029674\n",
      "Epoch:42=====Loss:0.001969475329387933\n",
      "Epoch:43=====Loss:0.001961602735100314\n",
      "Epoch:44=====Loss:0.0019076389819383621\n",
      "Epoch:45=====Loss:0.0018908733723219484\n",
      "Epoch:46=====Loss:0.0018872638524044306\n",
      "Epoch:47=====Loss:0.0018464001873508096\n",
      "Epoch:48=====Loss:0.0017926154122687877\n",
      "Epoch:49=====Loss:0.001749275578185916\n",
      "Epoch:50=====Loss:0.0018106246250681579\n"
     ]
    }
   ],
   "source": [
    "train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "99e3a269-c654-465e-ab19-c85d0aa16d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a90a96-c758-401f-acef-6ccd1e71634c",
   "metadata": {},
   "source": [
    "### Section III: Settings for the ManeFrame II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f39b88-bad3-4b1f-aab4-edc2daf841bd",
   "metadata": {},
   "source": [
    "Built the following Conda environment\n",
    "\n",
    "    module load spack\n",
    "\n",
    "    module load gcc-9.2\n",
    "\n",
    "    module load python/3\n",
    "\n",
    "    module load cuda-11.4.2-gcc-9.2.0-nxqfgxj \n",
    "\n",
    "    module load cudnn-8.2.4.15-11.4-gcc-9.2.0-tfeuowy\n",
    "\n",
    " \n",
    "\n",
    "    conda create --name **Env(Name of Environment)** numpy matplotlib jupyterlab(package you want) python=3.9(python version) --channel conda-forge\n",
    "\n",
    "    conda activate Env\n",
    "\n",
    "    pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    " \n",
    "\n",
    "You’ll need to load the modules before you run everything. The cuda libraries are installed with spack, so loading spack makes them show up as available. Gcc is technically not necessary, but the default system compiler is pretty old and sometimes causes problem so I like to load a newer compiler. The python/3 module is to make a version of Conda available (you can install your own Miniconda or similar if you want more control.) Finally, cuda and cudnn are needed by tensorflow and Jax to use the GPUs (I believe these are the newest versions our current drivers will support).\n",
    "\n",
    " \n",
    "\n",
    "If you are using the web portal, then you’ll want to put something like the following in the “Custom environment settings”\n",
    "\n",
    " \n",
    "\n",
    "    module load spack\n",
    "\n",
    "    module load gcc-9.2\n",
    "\n",
    "    module load python/3\n",
    "\n",
    "    module load cuda-11.4.2-gcc-9.2.0-nxqfgxj\n",
    "\n",
    "    module load cudnn-8.2.4.15-11.4-gcc-9.2.0-tfeuowy\n",
    "\n",
    "    eval \"$(conda shell.bash hook)\"\n",
    "\n",
    "    conda activate Env\n",
    "\n",
    " \n",
    "\n",
    "The “eval” line makes sure the conda commands are available and then the last line activates the conda environment build above (feel free to change the name to whatever you’d like.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6b514e-3509-4dad-8b9c-6aaf5d1c50a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
